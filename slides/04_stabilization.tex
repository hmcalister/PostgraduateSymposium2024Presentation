\section{Stabilizing the Network}

% --------------------------------------------------------------------------------

% \begin{frame}
% \frametitle{Stabilizing the Network}

% \begin{itemize}
%     \item The optimal hyperparameter region shrinks as \(n\) grows.
%     \item The optimal region also shifts considerably as \(n\) grows.
%     \item When \(n\) reaches some critical threshold, the optimal region is disrupted and the network does not learn at all.
% \end{itemize}

% \end{frame}

%------------------------------------------------

\begin{frame}
    \frametitle{Network Dynamics Step by Step}
    \begin{gather*}
        \tanh \left[ \beta \sum_{\mu} \left( f_n \left(\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) - f_n \left( -\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) \right) \right]
    \end{gather*}

    \begin{enumerate}
        \only<1,2>{\item Calculate similarities \(\zeta \cdot \xi_{+1}, \; \zeta \cdot \xi_{-1}\)}
        \only<3->{\item \textbf{Calculate similarities \(\zeta \cdot \xi_{+1}, \; \zeta \cdot \xi_{-1}\)}}
        \only<1>{\item Pass similarities through interaction function \(f_n\)}
        \only<2->{\item \textbf{Pass similarities through interaction function \(f_n\)}}
        \item Sum the result over all memories \(\sum_\mu\)
        \item Multiply by a scaling factor \(\beta\)
        \item Pass through activation function (e.g. Sign or tanh)
    \end{enumerate}

    \only<2>{
        \begin{align*}
            f_n\left(x\right) = x^n
        \end{align*}
    }

    \only<3>{
        \begin{align*}
            \zeta, \xi \in [-1,1]^N &\implies \zeta \cdot \xi \in [-N, N] \\
        \end{align*}
    }

    \only<4>{
        \begin{align*}
            \zeta, \xi \in [-1,1]^N &\implies \zeta \cdot \xi \in [-N, N] \\
            f_n\left(\zeta \cdot \xi\right) &= \left(\zeta \cdot \xi\right)^n 
        \end{align*}
    }

    \only<5>{
        \begin{align*}
            \zeta, \xi \in [-1,1]^N &\implies \zeta \cdot \xi \in [-N, N] \\
            f_n\left(\zeta \cdot \xi\right) &= \left(\zeta \cdot \xi\right)^n \\
                &= N^n
        \end{align*}
    }

\end{frame}

\begin{frame}
    \frametitle{How large is too large?}

    \begin{center}
        \begin{tabular}{| c | c |}
            \hline
            Network parameters & Interaction function value \\
            \hline
            \hline
            \(N=100, n=2\) & \(10^{4}\) \\
            \hline
            \(N=100, n=5\) & \(10^{10}\) \\
            \hline
            \(N=100, n=10\) & \(10^{18.89}\) \\
            \hline
            \(N=100, n=20\) & \(10^{40}\) \\
            \hline
        \end{tabular}
    \end{center}

    \pause
    Maximum value of a float32 is \(\approx 3.4 \cdot 10^{38}\). Default data type of PyTorch.

    \pause
    Maximum value of a float64 is \(\approx 1.8 \cdot 10^{304}\). Default data type of numpy.
\end{frame}

\begin{frame}
    \frametitle{Stabilizing the Network}
    
    \begin{itemize}
        \item The optimal hyperparameter region shrinks as \(n\) grows.
        \pause
        \item The optimal region also shifts considerably as \(n\) grows.
        \pause
        \item When \(n\) reaches some critical threshold, the optimal region is disrupted and the network does not learn at all.
        \pause
        \begin{itemize}
            \item This is because the memories are learned to be prototypes for large \(n\), which have large similarities scores with the probe states, and are made larger by the interaction function.
        \end{itemize}
    \end{itemize}    
\end{frame}
    

\begin{frame}
    \frametitle{Fixing the Problem}
    \only<1>{
        \begin{gather*}
            \tanh \left[ \beta \sum_{\mu} \left( f_n \left(\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) - f_n \left( -\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) \right) \right]
        \end{gather*}
    }
    \only<2>{
        \begin{gather*}
            \tanh \left[ \textcolor{red}{\boldsymbol \beta} \sum_{\mu} \left( f_n \left(\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) - f_n \left( -\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) \right) \right]
        \end{gather*}
    }
    \only<3>{
        \begin{gather*}
            \tanh \left[ \sum_{\mu} \left( \textcolor{red}{\boldsymbol \beta} f_n \left(\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) - \textcolor{red}{\boldsymbol \beta} f_n \left( -\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) \right) \right]
        \end{gather*}
    }
    \only<4>{
        \begin{gather*}
            \tanh \left[ \sum_{\mu} \left( f_n \left(\textcolor{red}{\boldsymbol \beta} \left(\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) \right) - f_n \left(\textcolor{red}{\boldsymbol \beta} \left( -\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) \right) \right) \right]
        \end{gather*}
        \begin{center}
            If \(f_n\) is homogenous: \(f(\alpha x) = \alpha^k f(x)\). \\
            Weaker than linear -- includes Polynomial and Rectified Polynomial.
        \end{center}
    }

    \only<5>{
        \begin{gather*}
            \tanh \left[ \sum_{\mu} \left( f_n \left(\textcolor{red}{\frac{\boldsymbol \beta}{N}} \left(\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) \right) - f_n \left(\textcolor{red}{\frac{\boldsymbol \beta}{N}} \left( -\zeta_{\mu, i} + \sum_{j \neq i} \zeta_{\mu, j} \xi_{j} \right) \right) \right) \right]
        \end{gather*}
        \begin{center}
            If \(f_n\) is homogenous: \(f(\alpha x) = \alpha^k f(x)\). \\
            Weaker than linear -- includes Polynomial and Rectified Polynomial.
        \end{center}
    }
\end{frame}
